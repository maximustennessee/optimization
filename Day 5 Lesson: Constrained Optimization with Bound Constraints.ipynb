{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5\n",
    "\n",
    "Learn how to solve optimization problems when the decision variables must lie within specified bounds. Bound constraints restrict the range of each variable (e.g., lower and upper limits) and are common in real-world problems. In this lesson, you’ll use SciPy’s minimize function with bound constraints.\n",
    "\n",
    "### Bound Constraints:\n",
    "These specify limits for the variables. For a one-dimensional variable x, a bound might be a≤x≤b.\n",
    "\n",
    "### SciPy’s minimize with Bounds:\n",
    "The minimize function accepts a bounds parameter. For a variable x (or a vector of variables), you provide a list of tuples indicating each variable’s  (lower bound,upper bound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to minimize\n",
    "\n",
    "def f(x):\n",
    "    return (x - 6) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounds\n",
    "\n",
    "bounds = [(0,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take an initial guess and solve\n",
    "\n",
    "initial_guess = [1]\n",
    "result = minimize(f, initial_guess, bounds=bounds, method=\"L-BFGS-B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal x: [5.]\n",
      "Minimum f(x): 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal x:\", result.x)\n",
    "print(\"Minimum f(x):\", result.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available algorithms for minimize\n",
    "\n",
    "Below is an explanation of several key algorithms/methods available in SciPy's minimize function, along with when you might choose each:\n",
    "\n",
    "1. Nelder-Mead\n",
    "Type: Derivative-free (Simplex method)\n",
    "Usage:\n",
    "Useful when the function is noisy, discontinuous, or you don't have gradient information.\n",
    "It explores the space using a simplex (a geometric shape with n+1 vertices in n dimensions).\n",
    "Pros/Cons:\n",
    "Pros: No need for gradients.\n",
    "Cons: Can be slow for high-dimensional problems and might not converge to the exact optimum.\n",
    "2. Powell\n",
    "Type: Derivative-free (Directional set methods)\n",
    "Usage:\n",
    "Optimizes by performing a series of one-dimensional minimizations along chosen directions.\n",
    "Does not require gradient information.\n",
    "Pros/Cons:\n",
    "Pros: Useful when derivatives are hard to compute.\n",
    "Cons: May be slower and sometimes less robust compared to gradient-based methods on smooth problems.\n",
    "3. CG (Conjugate Gradient)\n",
    "Type: Gradient-based\n",
    "Usage:\n",
    "Suitable for large-scale unconstrained problems.\n",
    "Uses the gradient information to generate conjugate directions for faster convergence.\n",
    "Pros/Cons:\n",
    "Pros: Efficient for smooth problems.\n",
    "Cons: Only applicable to unconstrained or lightly constrained problems.\n",
    "4. BFGS\n",
    "Type: Gradient-based (Quasi-Newton)\n",
    "Usage:\n",
    "Approximates the Hessian (the matrix of second derivatives) using gradient evaluations.\n",
    "Good for smooth functions where curvature information helps speed up convergence.\n",
    "Pros/Cons:\n",
    "Pros: Faster convergence on smooth problems.\n",
    "Cons: Requires storing and updating the Hessian approximation, which can be memory-intensive for very large problems.\n",
    "5. L-BFGS-B\n",
    "Type: Gradient-based (Limited-memory quasi-Newton)\n",
    "Usage:\n",
    "A variant of BFGS that uses limited memory to handle high-dimensional problems.\n",
    "Supports bound constraints, making it ideal when you need to restrict variable ranges.\n",
    "Pros/Cons:\n",
    "Pros: Efficient for large-scale problems with bounds.\n",
    "Cons: Less suited for problems with complex constraints beyond simple bounds.\n",
    "6. TNC (Truncated Newton Conjugate-Gradient)\n",
    "Type: Gradient-based (Newton method with conjugate gradients)\n",
    "Usage:\n",
    "Uses an approximation of the Hessian and supports bound constraints.\n",
    "Well-suited for large-scale optimization where the Hessian is difficult to compute explicitly.\n",
    "Pros/Cons:\n",
    "Pros: Can handle large problems with bounds.\n",
    "Cons: More complex to configure and tune.\n",
    "7. COBYLA (Constrained Optimization BY Linear Approximations)\n",
    "Type: Derivative-free\n",
    "Usage:\n",
    "Designed to handle constraints (specifically inequality constraints).\n",
    "Works by approximating the feasible region with linear approximations.\n",
    "Pros/Cons:\n",
    "Pros: Useful when derivatives are unavailable or unreliable.\n",
    "Cons: Does not support equality constraints and may converge slowly on some problems.\n",
    "8. SLSQP (Sequential Least Squares Programming)\n",
    "Type: Gradient-based\n",
    "Usage:\n",
    "Can handle both equality and inequality constraints.\n",
    "Solves the optimization problem by sequentially solving quadratic programming approximations.\n",
    "Pros/Cons:\n",
    "Pros: Flexible for constrained optimization problems; good when derivatives are available.\n",
    "Cons: May be sensitive to the initial guess and constraint formulation.\n",
    "9. trust-constr\n",
    "Type: Trust-region method for constrained optimization\n",
    "Usage:\n",
    "Designed for large-scale constrained problems.\n",
    "Supports both inequality and equality constraints and can use second-order derivative information when available.\n",
    "Pros/Cons:\n",
    "Pros: Robust and effective for complex constraint structures.\n",
    "Cons: Computationally more intensive and may require tuning of trust-region parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Method\n",
    "Bound Constraints:\n",
    "If your problem only has bound constraints, L-BFGS-B is a popular choice.\n",
    "General Constraints:\n",
    "For problems with both equality and inequality constraints, SLSQP and trust-constr are commonly used.\n",
    "Derivative-Free Cases:\n",
    "If gradients are unavailable or unreliable, methods like Nelder-Mead, Powell, or COBYLA might be more appropriate.\n",
    "Large-Scale Problems:\n",
    "For high-dimensional problems, limited-memory methods like L-BFGS-B or conjugate gradient approaches (CG, TNC) are preferred."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
